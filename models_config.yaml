# Vision Task Configuration
vision_config:
  data_path_facebook: "data/raw/facebook-hateful-memes"
  labels_relative_location_facebook: 
    - "fine_grained_labels/dev_unseen.jsonl"
    - "fine_grained_labels/dev_seen.jsonl"
  data_path_multioff: "data/interim/MultiOFF_Dataset"
  labels_relative_location_multioff: 
    - "Split Dataset/Training_meme_dataset.csv"
    - "Split Dataset/Validation_meme_dataset.csv"
    - "Split Dataset/Testing_meme_dataset.csv"
  extreme_pos_path: "data/results/extreme_pos_personas/[MODEL_NAME]/[TYPE].pkl"
  output_path: "data/results/img_classification/[MODEL_NAME]/[DATETIME].json"
  batch_size: 2000
  num_workers: 2
  dataset_seed: 22

  models:
    - name: "HuggingFaceM4/Idefics3-8B-Llama3"
      vllm_seed: 22
      temperature: 0.0
      max_model_len: 3000  #Limits Context Window (8192 suggested default for all models but might be too big for what I need)
      max_num_seqs: 1000    #Number of concurrent requests in a batch. Keep it higher than the number shown by vllm
      enforce_eager: False
      resolution_factor: 3 # Idefics3 often uses a resolution factor, e.g., N=4 for 364x364 images
      max_tokens: 200 # Output length
    - name: "Qwen/Qwen2.5-VL-7B-Instruct"
      vllm_seed: 22
      temperature: 0.0
      max_model_len: 2048 
      max_num_seqs: 500
      enforce_eager: False
      resolution_factor: null # Qwen VL models use min_pixels/max_pixels or exact dimensions
      max_tokens: 200
    - name: "Qwen/Qwen2.5-VL-32B-Instruct"
      vllm_seed: 22
      temperature: 0.0
      max_model_len: 2048
      max_num_seqs: 1000
      enforce_eager: False
      resolution_factor: null # Qwen VL models use min_pixels/max_pixels or exact dimensions
      max_tokens: 200
    # - name: "openbmb/MiniCPM-o-2_6" # 9B parameters
    #   vllm_seed: 22
    #   temperature: 0.0
    #   max_model_len: 2048
    #   max_num_seqs: 1000
    #   enforce_eager: False
    #   resolution_factor: null # MiniCPM-o uses min_pixels/max_pixels or exact dimensions
    #   max_tokens: 200

# Text Task Configuration (from your previous example, with adjustments)
text_config:
  data_path_yoder: "data/raw/yoder_data/sampled/identity_hate_corpora_with_id.jsonl"
  data_path_subdata: "data/raw/subdata/political_complete.csv"
  data_path_cad: "data/processed/cad/cad_v1_1_processed.tsv"
  extreme_pos_path: "data/results/extreme_pos_personas/[MODEL_NAME]/[TYPE].pkl"
  output_path: "data/results/text_classification/[MODEL_NAME]/[DATETIME]"
  fold: "test"
  target_group_size: 1 # keep only rows were target group size is 1
  batch_size: 50000 # 50000
  num_workers: 5
  dataset_seed: 22

  models:
    - name: "meta-llama/Llama-3.1-8B-Instruct"
      vllm_seed: 22
      temperature: 0.0
      max_model_len: 2048
      max_num_seqs: 1000
      enforce_eager: False
      max_tokens: 150
    - name: "meta-llama/Llama-3.1-70B-Instruct"
      vllm_seed: 22
      temperature: 0.0
      max_model_len: 2048
      max_num_seqs: 1000
      tensor_parallel_size: 2
      enforce_eager: False
      max_tokens: 150
    - name: "Qwen/Qwen2.5-32B-Instruct"
      vllm_seed: 22
      temperature: 0.0
      max_model_len: 2048
      max_num_seqs: 500
      enforce_eager: False
      max_tokens: 150