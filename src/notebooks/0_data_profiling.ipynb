{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0a1681",
   "metadata": {},
   "source": [
    "# Yoder Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5ddc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 276872 records from Demographic_category_hate.jsonl\n",
      "Loaded 159872 records from Identity_hate_corpora.jsonl\n",
      "Loaded 295016 records from Power_hate_corpora.jsonl\n",
      "Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries and load data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the datasets\n",
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load JSONL file into a list of dictionaries\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load both datasets\n",
    "demographic_data = load_jsonl('../../data/raw/yoder_data/sampled/demographic_category_hate_corpora.jsonl')\n",
    "identity_data = load_jsonl('../../data/raw/yoder_data/sampled/identity_hate_corpora.jsonl')\n",
    "power_data = load_jsonl('../../data/raw/yoder_data/sampled/power_hate_corpora.jsonl')\n",
    "\n",
    "print(f\"Loaded {len(demographic_data)} records from Demographic_category_hate.jsonl\")\n",
    "print(f\"Loaded {len(identity_data)} records from Identity_hate_corpora.jsonl\")\n",
    "print(f\"Loaded {len(power_data)} records from Power_hate_corpora.jsonl\")\n",
    "print(\"Data loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58f78ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET STRUCTURE ANALYSIS ===\n",
      "\n",
      "DEMOGRAPHIC DATASET:\n",
      "Shape: (276872, 6)\n",
      "Columns: ['grouping', 'fold', 'text', 'target_groups', 'dataset', 'hate']\n",
      "Data types:\n",
      "grouping         object\n",
      "fold             object\n",
      "text             object\n",
      "target_groups    object\n",
      "dataset          object\n",
      "hate               bool\n",
      "dtype: object\n",
      "\n",
      "IDENTITY DATASET:\n",
      "Shape: (159872, 6)\n",
      "Columns: ['grouping', 'fold', 'text', 'target_groups', 'dataset', 'hate']\n",
      "Data types:\n",
      "grouping         object\n",
      "fold             object\n",
      "text             object\n",
      "target_groups    object\n",
      "dataset          object\n",
      "hate               bool\n",
      "dtype: object\n",
      "\n",
      "POWER DATASET:\n",
      "Shape: (295016, 6)\n",
      "Columns: ['grouping', 'fold', 'text', 'target_groups', 'dataset', 'hate']\n",
      "Data types:\n",
      "grouping         object\n",
      "fold             object\n",
      "text             object\n",
      "target_groups    object\n",
      "dataset          object\n",
      "hate               bool\n",
      "dtype: object\n",
      "\n",
      "MISSING VALUES:\n",
      "Demographic dataset:\n",
      "grouping         0\n",
      "fold             0\n",
      "text             0\n",
      "target_groups    0\n",
      "dataset          0\n",
      "hate             0\n",
      "dtype: int64\n",
      "\n",
      "Identity dataset:\n",
      "grouping         0\n",
      "fold             0\n",
      "text             0\n",
      "target_groups    0\n",
      "dataset          0\n",
      "hate             0\n",
      "dtype: int64\n",
      "\n",
      "Power dataset:\n",
      "grouping         0\n",
      "fold             0\n",
      "text             0\n",
      "target_groups    0\n",
      "dataset          0\n",
      "hate             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Basic data structure analysis\n",
    "print(\"=== DATASET STRUCTURE ANALYSIS ===\\n\")\n",
    "\n",
    "# Convert to DataFrames for easier analysis\n",
    "df_demo = pd.DataFrame(demographic_data)\n",
    "df_identity = pd.DataFrame(identity_data)\n",
    "df_power = pd.DataFrame(power_data)\n",
    "\n",
    "print(\"DEMOGRAPHIC DATASET:\")\n",
    "print(f\"Shape: {df_demo.shape}\")\n",
    "print(f\"Columns: {list(df_demo.columns)}\")\n",
    "print(f\"Data types:\\n{df_demo.dtypes}\\n\")\n",
    "\n",
    "print(\"IDENTITY DATASET:\")\n",
    "print(f\"Shape: {df_identity.shape}\")\n",
    "print(f\"Columns: {list(df_identity.columns)}\")\n",
    "print(f\"Data types:\\n{df_identity.dtypes}\\n\")\n",
    "\n",
    "print(\"POWER DATASET:\")\n",
    "print(f\"Shape: {df_power.shape}\")\n",
    "print(f\"Columns: {list(df_power.columns)}\")\n",
    "print(f\"Data types:\\n{df_power.dtypes}\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"MISSING VALUES:\")\n",
    "print(\"Demographic dataset:\")\n",
    "print(df_demo.isnull().sum())\n",
    "print(\"\\nIdentity dataset:\")\n",
    "print(df_identity.isnull().sum())\n",
    "print(\"\\nPower dataset:\")\n",
    "print(df_power.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed9f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mistral and llama tokenizer to count tokens of the longest text in each dataset\n",
    "def count_tokens(text, tokenizer):\n",
    "    \"\"\"Count tokens in a text using the specified tokenizer.\"\"\"\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "from transformers import AutoTokenizer\n",
    "# Load tokenizers\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# Count tokens in the longest text of each dataset\n",
    "longest_demo_text = max(df_demo['text'], key=lambda x: count_tokens(x, llama_tokenizer))\n",
    "longest_identity_text = max(df_identity['text'], key=lambda x: count_tokens(x, llama_tokenizer))\n",
    "longest_power_text = max(df_power['text'], key=lambda x: count_tokens(x, llama_tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857822d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(longest_identity_text, llama_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e3b6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grouping</th>\n",
       "      <th>fold</th>\n",
       "      <th>text</th>\n",
       "      <th>target_groups</th>\n",
       "      <th>dataset</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>race/ethnicity</td>\n",
       "      <td>train</td>\n",
       "      <td>RIP, Captain.  My condolences to his wife, fam...</td>\n",
       "      <td>[]</td>\n",
       "      <td>civilcomments</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>race/ethnicity</td>\n",
       "      <td>train</td>\n",
       "      <td>There's nothing wrong with sex between two con...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sbic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>race/ethnicity</td>\n",
       "      <td>train</td>\n",
       "      <td>Ultimately a cis gay man (like me for the reco...</td>\n",
       "      <td>[men, transgender men, transgender women, wome...</td>\n",
       "      <td>kennedy2020</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>race/ethnicity</td>\n",
       "      <td>train</td>\n",
       "      <td>There most certainly are some words that I dou...</td>\n",
       "      <td>[]</td>\n",
       "      <td>civilcomments</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>race/ethnicity</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; &lt;user&gt; &lt;user&gt; &lt;user&gt; a vo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>hatexplain</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276867</th>\n",
       "      <td>gender/sexuality</td>\n",
       "      <td>test</td>\n",
       "      <td>who the best lesbian character of all time and...</td>\n",
       "      <td>[women, lgbtq+ people]</td>\n",
       "      <td>hatexplain</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276868</th>\n",
       "      <td>gender/sexuality</td>\n",
       "      <td>test</td>\n",
       "      <td>🔥ANAL🔥 Daddy's cum is my favorite lube. Click ...</td>\n",
       "      <td>[men, transgender men, transgender women, wome...</td>\n",
       "      <td>kennedy2020</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276869</th>\n",
       "      <td>gender/sexuality</td>\n",
       "      <td>test</td>\n",
       "      <td>fuck you you bitch ass nigger cunt i hate blac...</td>\n",
       "      <td>[black people, women, men]</td>\n",
       "      <td>hatexplain</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276870</th>\n",
       "      <td>gender/sexuality</td>\n",
       "      <td>test</td>\n",
       "      <td>THEY CAN BE WHOEVER YOU WANT TO BE IF I WAS A ...</td>\n",
       "      <td>[men, women, bisexual people, gay people]</td>\n",
       "      <td>kennedy2020</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276871</th>\n",
       "      <td>gender/sexuality</td>\n",
       "      <td>test</td>\n",
       "      <td>My short time here has already blackpilled me ...</td>\n",
       "      <td>[women]</td>\n",
       "      <td>sbic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276872 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                grouping   fold  \\\n",
       "0         race/ethnicity  train   \n",
       "1         race/ethnicity  train   \n",
       "2         race/ethnicity  train   \n",
       "3         race/ethnicity  train   \n",
       "4         race/ethnicity  train   \n",
       "...                  ...    ...   \n",
       "276867  gender/sexuality   test   \n",
       "276868  gender/sexuality   test   \n",
       "276869  gender/sexuality   test   \n",
       "276870  gender/sexuality   test   \n",
       "276871  gender/sexuality   test   \n",
       "\n",
       "                                                     text  \\\n",
       "0       RIP, Captain.  My condolences to his wife, fam...   \n",
       "1       There's nothing wrong with sex between two con...   \n",
       "2       Ultimately a cis gay man (like me for the reco...   \n",
       "3       There most certainly are some words that I dou...   \n",
       "4       <user> <user> <user> <user> <user> <user> a vo...   \n",
       "...                                                   ...   \n",
       "276867  who the best lesbian character of all time and...   \n",
       "276868  🔥ANAL🔥 Daddy's cum is my favorite lube. Click ...   \n",
       "276869  fuck you you bitch ass nigger cunt i hate blac...   \n",
       "276870  THEY CAN BE WHOEVER YOU WANT TO BE IF I WAS A ...   \n",
       "276871  My short time here has already blackpilled me ...   \n",
       "\n",
       "                                            target_groups        dataset  \\\n",
       "0                                                      []  civilcomments   \n",
       "1                                                      []           sbic   \n",
       "2       [men, transgender men, transgender women, wome...    kennedy2020   \n",
       "3                                                      []  civilcomments   \n",
       "4                                                      []     hatexplain   \n",
       "...                                                   ...            ...   \n",
       "276867                             [women, lgbtq+ people]     hatexplain   \n",
       "276868  [men, transgender men, transgender women, wome...    kennedy2020   \n",
       "276869                         [black people, women, men]     hatexplain   \n",
       "276870          [men, women, bisexual people, gay people]    kennedy2020   \n",
       "276871                                            [women]           sbic   \n",
       "\n",
       "         hate  \n",
       "0       False  \n",
       "1       False  \n",
       "2       False  \n",
       "3       False  \n",
       "4       False  \n",
       "...       ...  \n",
       "276867  False  \n",
       "276868  False  \n",
       "276869   True  \n",
       "276870  False  \n",
       "276871   True  \n",
       "\n",
       "[276872 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7503b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test records in demographic dataset: 110736, \n",
      "Number of test records in identity dataset: 63952\n",
      "\n",
      "Number of test records in power dataset: 118160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo_test = df_demo[df_demo['fold'] == 'test']\n",
    "df_identity_test = df_identity[df_identity['fold'] == 'test']\n",
    "df_power_test = df_power[df_power['fold'] == 'test']\n",
    "\n",
    "print(f'Number of test records in demographic dataset: {len(df_demo_test)}, ')\n",
    "print(f'Number of test records in identity dataset: {len(df_identity_test)}\\n')\n",
    "print(f'Number of test records in power dataset: {len(df_power_test)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619bc0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows with less than one value in targeted groups:\n",
      "Demographic dataset: 105046\n",
      "Identity dataset: 60566\n",
      "Power dataset: 111655\n"
     ]
    }
   ],
   "source": [
    "# number of rows with only less than one value in targeted groups\n",
    "print(\"\\nRows with less than one value in targeted groups:\")\n",
    "print(\"Demographic dataset:\", df_demo[df_demo['target_groups'].apply(lambda x: len(x) < 1)].shape[0])\n",
    "print(\"Identity dataset:\", df_identity[(df_identity['target_groups'].apply(lambda x: len(x) < 1))].shape[0])\n",
    "print(\"Power dataset:\", df_power[df_power['target_groups'].apply(lambda x: len(x) < 1)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf04e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test records in identity dataset with single target group: 13846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_identity_test_singleTarget = df_identity_test[df_identity_test['target_groups'].apply(lambda x: len(x) == 1)]\n",
    "print(f'Number of test records in identity dataset with single target group: {len(df_identity_test_singleTarget)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac0da34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grouping</th>\n",
       "      <th>fold</th>\n",
       "      <th>text</th>\n",
       "      <th>target_groups</th>\n",
       "      <th>dataset</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>christians</td>\n",
       "      <td>test</td>\n",
       "      <td>these niggas ungrateful</td>\n",
       "      <td>[]</td>\n",
       "      <td>hatexplain</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>christians</td>\n",
       "      <td>test</td>\n",
       "      <td>I hate girls who call everyone their bestfrien...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sbic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>christians</td>\n",
       "      <td>test</td>\n",
       "      <td>This girl is complaining that her grandma keep...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sbic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>christians</td>\n",
       "      <td>test</td>\n",
       "      <td>It's sad when the stupid petitions get filled ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sbic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>christians</td>\n",
       "      <td>test</td>\n",
       "      <td>I don't like him, cancer can fuck off but I wo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sbic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>christians</td>\n",
       "      <td>test</td>\n",
       "      <td>Y'all be cool out there in that cold chasing t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sbic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>christians</td>\n",
       "      <td>test</td>\n",
       "      <td>Already winning on my birthday, because I just...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sbic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>christians</td>\n",
       "      <td>test</td>\n",
       "      <td>How DARE you go and HUNT the bear down!!! If a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>civilcomments</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>christians</td>\n",
       "      <td>test</td>\n",
       "      <td>Hi guys! Any updates on the Ottawa game?? Wait...</td>\n",
       "      <td>[]</td>\n",
       "      <td>civilcomments</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>christians</td>\n",
       "      <td>test</td>\n",
       "      <td>Women whose third child is born following a se...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sbic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        grouping  fold                                               text  \\\n",
       "1816  christians  test                            these niggas ungrateful   \n",
       "1817  christians  test  I hate girls who call everyone their bestfrien...   \n",
       "1820  christians  test  This girl is complaining that her grandma keep...   \n",
       "1825  christians  test  It's sad when the stupid petitions get filled ...   \n",
       "1834  christians  test  I don't like him, cancer can fuck off but I wo...   \n",
       "1835  christians  test  Y'all be cool out there in that cold chasing t...   \n",
       "1840  christians  test  Already winning on my birthday, because I just...   \n",
       "1843  christians  test  How DARE you go and HUNT the bear down!!! If a...   \n",
       "1845  christians  test  Hi guys! Any updates on the Ottawa game?? Wait...   \n",
       "1848  christians  test  Women whose third child is born following a se...   \n",
       "\n",
       "     target_groups        dataset   hate  \n",
       "1816            []     hatexplain  False  \n",
       "1817            []           sbic  False  \n",
       "1820            []           sbic  False  \n",
       "1825            []           sbic  False  \n",
       "1834            []           sbic  False  \n",
       "1835            []           sbic  False  \n",
       "1840            []           sbic  False  \n",
       "1843            []  civilcomments  False  \n",
       "1845            []  civilcomments  False  \n",
       "1848            []           sbic  False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_identity_test_singleTarget.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be57fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07784874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_identity.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fee719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Hate label distribution analysis\n",
    "print(\"=== HATE LABEL DISTRIBUTION ===\\n\")\n",
    "\n",
    "# Analyze hate label distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "\n",
    "# Demographic dataset\n",
    "demo_hate_counts = df_demo['hate'].value_counts()\n",
    "axes[0].pie(demo_hate_counts.values, labels=demo_hate_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Demographic Dataset\\nHate Label Distribution')\n",
    "\n",
    "# Identity dataset\n",
    "identity_hate_counts = df_identity['hate'].value_counts()\n",
    "axes[1].pie(identity_hate_counts.values, labels=identity_hate_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Identity Dataset\\nHate Label Distribution')\n",
    "\n",
    "# Power dataset\n",
    "power_hate_counts = df_power['hate'].value_counts()\n",
    "axes[2].pie(power_hate_counts.values, labels=power_hate_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[2].set_title('Power Dataset\\nHate Label Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"DEMOGRAPHIC DATASET:\")\n",
    "print(f\"Total records: {len(df_demo)}\")\n",
    "print(f\"Hate: {demo_hate_counts.get(True, 0)} ({demo_hate_counts.get(True, 0)/len(df_demo)*100:.1f}%)\")\n",
    "print(f\"Non-hate: {demo_hate_counts.get(False, 0)} ({demo_hate_counts.get(False, 0)/len(df_demo)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nIDENTITY DATASET:\")\n",
    "print(f\"Total records: {len(df_identity)}\")\n",
    "print(f\"Hate: {identity_hate_counts.get(True, 0)} ({identity_hate_counts.get(True, 0)/len(df_identity)*100:.1f}%)\")\n",
    "print(f\"Non-hate: {identity_hate_counts.get(False, 0)} ({identity_hate_counts.get(False, 0)/len(df_identity)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPOWER DATASET:\")\n",
    "print(f\"Total records: {len(df_power)}\")\n",
    "print(f\"Hate: {power_hate_counts.get(True, 0)} ({power_hate_counts.get(True, 0)/len(df_power)*100:.1f}%)\")\n",
    "print(f\"Non-hate: {power_hate_counts.get(False, 0)} ({power_hate_counts.get(False, 0)/len(df_power)*100:.1f}%)\")\n",
    "\n",
    "# Combined statistics\n",
    "total_records = len(df_demo) + len(df_identity) + len(df_power)\n",
    "total_hate = demo_hate_counts.get(True, 0) + identity_hate_counts.get(True, 0) + power_hate_counts.get(True, 0)\n",
    "total_non_hate = demo_hate_counts.get(False, 0) + identity_hate_counts.get(False, 0) + power_hate_counts.get(False, 0)\n",
    "\n",
    "print(f\"\\nCOMBINED DATASETS:\")\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Total hate: {total_hate} ({total_hate/total_records*100:.1f}%)\")\n",
    "print(f\"Total non-hate: {total_non_hate} ({total_non_hate/total_records*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Grouping and dataset source analysis\n",
    "print(\"=== GROUPING ANALYSIS ===\\n\")\n",
    "\n",
    "# Analyze grouping categories\n",
    "print(\"DEMOGRAPHIC DATASET - Grouping categories:\")\n",
    "demo_grouping = df_demo['grouping'].value_counts()\n",
    "print(demo_grouping)\n",
    "\n",
    "print(\"\\nIDENTITY DATASET - Grouping categories:\")\n",
    "identity_grouping = df_identity['grouping'].value_counts()\n",
    "print(identity_grouping)\n",
    "\n",
    "print(\"\\nPOWER DATASET - Grouping categories:\")\n",
    "power_grouping = df_power['grouping'].value_counts()\n",
    "print(power_grouping)\n",
    "\n",
    "# Visualize grouping distribution\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "# Demographic dataset groupings\n",
    "demo_grouping.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Demographic Dataset - Grouping Distribution')\n",
    "axes[0].set_xlabel('Grouping Category')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Identity dataset groupings\n",
    "identity_grouping.plot(kind='bar', ax=axes[1], color='lightcoral')\n",
    "axes[1].set_title('Identity Dataset - Grouping Distribution')\n",
    "axes[1].set_xlabel('Grouping Category')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Power dataset groupings\n",
    "power_grouping.plot(kind='bar', ax=axes[2], color='lightgreen')\n",
    "axes[2].set_title('Power Dataset - Grouping Distribution')\n",
    "axes[2].set_xlabel('Grouping Category') \n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Dataset source analysis\n",
    "print(\"\\n=== DATASET SOURCE ANALYSIS ===\\n\")\n",
    "\n",
    "print(\"DEMOGRAPHIC DATASET - Dataset sources:\")\n",
    "demo_datasets = df_demo['dataset'].value_counts()\n",
    "print(demo_datasets)\n",
    "\n",
    "print(\"\\nIDENTITY DATASET - Dataset sources:\")\n",
    "identity_datasets = df_identity['dataset'].value_counts()\n",
    "print(identity_datasets)\n",
    "\n",
    "print(\"\\nPOWER DATASET - Dataset sources:\")\n",
    "power_datasets = df_power['dataset'].value_counts()\n",
    "print(power_datasets)\n",
    "\n",
    "# Combined dataset sources\n",
    "all_datasets = pd.concat([df_demo['dataset'], df_identity['dataset'], df_power['dataset']]).value_counts()\n",
    "print(f\"\\nCOMBINED - All dataset sources:\")\n",
    "print(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def extract_unique_values(df, column_name):\n",
    "    unique_values = set()\n",
    "    \n",
    "    for item in df[column_name].dropna():\n",
    "        # Skip empty lists\n",
    "        if item == '[]' or item == []:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Try to parse if it's a string representation\n",
    "            if isinstance(item, str):\n",
    "                value_list = ast.literal_eval(item)\n",
    "            else:\n",
    "                value_list = item\n",
    "                \n",
    "            if value_list:  # Check if list is not empty\n",
    "                unique_values.update(value_list)\n",
    "                \n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Could not process value: {item}\\nError: {e}\")\n",
    "    \n",
    "    return unique_values\n",
    "\n",
    "\n",
    "demo = extract_unique_values(df_demo, 'target_groups')\n",
    "identity = extract_unique_values(df_identity, 'target_groups')\n",
    "\n",
    "print(\"\\nAll unique target values found:\")\n",
    "print(\"-\" * 30)\n",
    "all_values = sorted(demo.union(identity))\n",
    "for value in all_values:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec23f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50655003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Target groups analysis\n",
    "print(\"=== TARGET GROUPS ANALYSIS 2 ===\\n\")\n",
    "\n",
    "# Function to extract all target groups\n",
    "def extract_target_groups(df):\n",
    "    all_groups = []\n",
    "    for groups_list in df['target_groups']:\n",
    "        all_groups.extend(groups_list)\n",
    "    return all_groups\n",
    "\n",
    "# Extract target groups from both datasets\n",
    "demo_target_groups = extract_target_groups(df_demo)\n",
    "identity_target_groups = extract_target_groups(df_identity)\n",
    "\n",
    "print(\"DEMOGRAPHIC DATASET:\")\n",
    "print(f\"Total target group mentions: {len(demo_target_groups)}\")\n",
    "print(f\"Unique target groups: {len(set(demo_target_groups))}\")\n",
    "print(f\"Records with target groups: {sum(1 for groups in df_demo['target_groups'] if groups)}\")\n",
    "print(f\"Records without target groups: {sum(1 for groups in df_demo['target_groups'] if not groups)}\")\n",
    "\n",
    "print(\"\\nIDENTITY DATASET:\")\n",
    "print(f\"Total target group mentions: {len(identity_target_groups)}\")\n",
    "print(f\"Unique target groups: {len(set(identity_target_groups))}\")\n",
    "print(f\"Records with target groups: {sum(1 for groups in df_identity['target_groups'] if groups)}\")\n",
    "print(f\"Records without target groups: {sum(1 for groups in df_identity['target_groups'] if not groups)}\")\n",
    "\n",
    "# Most common target groups\n",
    "print(\"\\n=== MOST COMMON TARGET GROUPS ===\")\n",
    "\n",
    "demo_group_counts = Counter(demo_target_groups)\n",
    "identity_group_counts = Counter(identity_target_groups)\n",
    "\n",
    "print(\"\\nDEMOGRAPHIC DATASET - Top 10 target groups:\")\n",
    "for group, count in demo_group_counts.most_common(10):\n",
    "    print(f\"{group}: {count}\")\n",
    "\n",
    "print(\"\\nIDENTITY DATASET - Top 10 target groups:\")\n",
    "for group, count in identity_group_counts.most_common(10):\n",
    "    print(f\"{group}: {count}\")\n",
    "\n",
    "# Combined analysis\n",
    "all_target_groups = demo_target_groups + identity_target_groups\n",
    "all_group_counts = Counter(all_target_groups)\n",
    "\n",
    "print(f\"\\nCOMBINED - Top 15 target groups:\")\n",
    "for group, count in all_group_counts.most_common(15):\n",
    "    print(f\"{group}: {count}\")\n",
    "\n",
    "# Visualize top target groups\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_groups = dict(all_group_counts.most_common(15))\n",
    "plt.barh(list(top_groups.keys()), list(top_groups.values()))\n",
    "plt.title('Top 15 Most Common Target Groups (Combined Datasets)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdcaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Text length and fold distribution analysis\n",
    "print(\"=== TEXT LENGTH ANALYSIS ===\\n\")\n",
    "\n",
    "# Calculate text lengths\n",
    "df_demo['text_length'] = df_demo['text'].str.len()\n",
    "df_identity['text_length'] = df_identity['text'].str.len()\n",
    "\n",
    "# Text length statistics\n",
    "print(\"DEMOGRAPHIC DATASET - Text length statistics:\")\n",
    "print(df_demo['text_length'].describe())\n",
    "\n",
    "print(\"\\nIDENTITY DATASET - Text length statistics:\")\n",
    "print(df_identity['text_length'].describe())\n",
    "\n",
    "# Visualize text length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Text length histograms\n",
    "axes[0,0].hist(df_demo['text_length'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].set_title('Demographic Dataset - Text Length Distribution')\n",
    "axes[0,0].set_xlabel('Text Length (characters)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "axes[0,1].hist(df_identity['text_length'], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[0,1].set_title('Identity Dataset - Text Length Distribution')\n",
    "axes[0,1].set_xlabel('Text Length (characters)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Text length by hate label\n",
    "df_demo.boxplot(column='text_length', by='hate', ax=axes[1,0])\n",
    "axes[1,0].set_title('Demographic Dataset - Text Length by Hate Label')\n",
    "axes[1,0].set_xlabel('Hate Label')\n",
    "axes[1,0].set_ylabel('Text Length')\n",
    "\n",
    "df_identity.boxplot(column='text_length', by='hate', ax=axes[1,1])\n",
    "axes[1,1].set_title('Identity Dataset - Text Length by Hate Label')\n",
    "axes[1,1].set_xlabel('Hate Label')\n",
    "axes[1,1].set_ylabel('Text Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Fold distribution analysis\n",
    "print(\"\\n=== FOLD DISTRIBUTION ANALYSIS ===\\n\")\n",
    "\n",
    "print(\"DEMOGRAPHIC DATASET - Fold distribution:\")\n",
    "demo_fold = df_demo['fold'].value_counts()\n",
    "print(demo_fold)\n",
    "\n",
    "print(\"\\nIDENTITY DATASET - Fold distribution:\")\n",
    "identity_fold = df_identity['fold'].value_counts()\n",
    "print(identity_fold)\n",
    "\n",
    "# Visualize fold distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "demo_fold.plot(kind='pie', ax=axes[0], autopct='%1.1f%%')\n",
    "axes[0].set_title('Demographic Dataset - Fold Distribution')\n",
    "\n",
    "identity_fold.plot(kind='pie', ax=axes[1], autopct='%1.1f%%')\n",
    "axes[1].set_title('Identity Dataset - Fold Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Cross-tabulation analysis\n",
    "print(\"=== CROSS-TABULATION ANALYSIS ===\\n\")\n",
    "\n",
    "# Hate vs Grouping cross-tabulation\n",
    "print(\"DEMOGRAPHIC DATASET - Hate vs Grouping:\")\n",
    "demo_crosstab = pd.crosstab(df_demo['grouping'], df_demo['hate'], margins=True)\n",
    "print(demo_crosstab)\n",
    "\n",
    "print(\"\\nIDENTITY DATASET - Hate vs Grouping:\")\n",
    "identity_crosstab = pd.crosstab(df_identity['grouping'], df_identity['hate'], margins=True)\n",
    "print(identity_crosstab)\n",
    "\n",
    "# Hate vs Dataset source cross-tabulation\n",
    "print(\"\\n=== HATE VS DATASET SOURCE ===\\n\")\n",
    "\n",
    "print(\"DEMOGRAPHIC DATASET - Hate vs Dataset source:\")\n",
    "demo_dataset_crosstab = pd.crosstab(df_demo['dataset'], df_demo['hate'], margins=True)\n",
    "print(demo_dataset_crosstab)\n",
    "\n",
    "print(\"\\nIDENTITY DATASET - Hate vs Dataset source:\")\n",
    "identity_dataset_crosstab = pd.crosstab(df_identity['dataset'], df_identity['hate'], margins=True)\n",
    "print(identity_dataset_crosstab)\n",
    "\n",
    "# Visualize cross-tabulations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Demographic dataset - Grouping vs Hate\n",
    "demo_crosstab_pct = pd.crosstab(df_demo['grouping'], df_demo['hate'], normalize='index') * 100\n",
    "demo_crosstab_pct.plot(kind='bar', ax=axes[0,0], color=['lightblue', 'lightcoral'])\n",
    "axes[0,0].set_title('Demographic Dataset - Hate % by Grouping')\n",
    "axes[0,0].set_ylabel('Percentage')\n",
    "axes[0,0].legend(['Non-Hate', 'Hate'])\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Identity dataset - Grouping vs Hate\n",
    "identity_crosstab_pct = pd.crosstab(df_identity['grouping'], df_identity['hate'], normalize='index') * 100\n",
    "identity_crosstab_pct.plot(kind='bar', ax=axes[0,1], color=['lightblue', 'lightcoral'])\n",
    "axes[0,1].set_title('Identity Dataset - Hate % by Grouping')\n",
    "axes[0,1].set_ylabel('Percentage')\n",
    "axes[0,1].legend(['Non-Hate', 'Hate'])\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Dataset source analysis\n",
    "demo_dataset_pct = pd.crosstab(df_demo['dataset'], df_demo['hate'], normalize='index') * 100\n",
    "demo_dataset_pct.plot(kind='bar', ax=axes[1,0], color=['lightblue', 'lightcoral'])\n",
    "axes[1,0].set_title('Demographic Dataset - Hate % by Source')\n",
    "axes[1,0].set_ylabel('Percentage')\n",
    "axes[1,0].legend(['Non-Hate', 'Hate'])\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "identity_dataset_pct = pd.crosstab(df_identity['dataset'], df_identity['hate'], normalize='index') * 100\n",
    "identity_dataset_pct.plot(kind='bar', ax=axes[1,1], color=['lightblue', 'lightcoral'])\n",
    "axes[1,1].set_title('Identity Dataset - Hate % by Source')\n",
    "axes[1,1].set_ylabel('Percentage')\n",
    "axes[1,1].legend(['Non-Hate', 'Hate'])\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77009172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Summary report\n",
    "print(\"=\"*60)\n",
    "print(\"                    DATASET PROFILING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def print_section(title):\n",
    "    print(f\"\\n{'='*len(title)}\")\n",
    "    print(title)\n",
    "    print('='*len(title))\n",
    "\n",
    "print_section(\"OVERALL STATISTICS\")\n",
    "total_records = len(df_demo) + len(df_identity)\n",
    "total_hate = sum(df_demo['hate']) + sum(df_identity['hate'])\n",
    "total_non_hate = total_records - total_hate\n",
    "\n",
    "print(f\"Total Records: {total_records:,}\")\n",
    "print(f\"Hate Records: {total_hate:,} ({total_hate/total_records*100:.1f}%)\")\n",
    "print(f\"Non-Hate Records: {total_non_hate:,} ({total_non_hate/total_records*100:.1f}%)\")\n",
    "\n",
    "print_section(\"DATASET BREAKDOWN\")\n",
    "print(f\"Demographic Dataset: {len(df_demo):,} records\")\n",
    "print(f\"  - Hate: {sum(df_demo['hate']):,} ({sum(df_demo['hate'])/len(df_demo)*100:.1f}%)\")\n",
    "print(f\"  - Non-Hate: {len(df_demo)-sum(df_demo['hate']):,} ({(len(df_demo)-sum(df_demo['hate']))/len(df_demo)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nIdentity Dataset: {len(df_identity):,} records\")\n",
    "print(f\"  - Hate: {sum(df_identity['hate']):,} ({sum(df_identity['hate'])/len(df_identity)*100:.1f}%)\")\n",
    "print(f\"  - Non-Hate: {len(df_identity)-sum(df_identity['hate']):,} ({(len(df_identity)-sum(df_identity['hate']))/len(df_identity)*100:.1f}%)\")\n",
    "\n",
    "print_section(\"GROUPING CATEGORIES\")\n",
    "all_groupings = pd.concat([df_demo['grouping'], df_identity['grouping']]).value_counts()\n",
    "print(f\"Total unique groupings: {len(all_groupings)}\")\n",
    "for grouping, count in all_groupings.head(10).items():\n",
    "    print(f\"  {grouping}: {count:,} records\")\n",
    "\n",
    "print_section(\"DATA SOURCES\")\n",
    "all_sources = pd.concat([df_demo['dataset'], df_identity['dataset']]).value_counts()\n",
    "print(f\"Total unique data sources: {len(all_sources)}\")\n",
    "for source, count in all_sources.items():\n",
    "    print(f\"  {source}: {count:,} records\")\n",
    "\n",
    "print_section(\"TARGET GROUPS\")\n",
    "demo_target_groups = extract_target_groups(df_demo)\n",
    "identity_target_groups = extract_target_groups(df_identity)\n",
    "all_target_groups = demo_target_groups + identity_target_groups\n",
    "unique_targets = len(set(all_target_groups))\n",
    "\n",
    "print(f\"Total target group mentions: {len(all_target_groups):,}\")\n",
    "print(f\"Unique target groups: {unique_targets}\")\n",
    "print(f\"Records with target groups: {sum(1 for df in [df_demo, df_identity] for groups in df['target_groups'] if groups):,}\")\n",
    "\n",
    "print_section(\"TEXT CHARACTERISTICS\")\n",
    "all_text_lengths = list(df_demo['text_length']) + list(df_identity['text_length'])\n",
    "print(f\"Average text length: {np.mean(all_text_lengths):.1f} characters\")\n",
    "print(f\"Median text length: {np.median(all_text_lengths):.1f} characters\")\n",
    "print(f\"Shortest text: {min(all_text_lengths)} characters\")\n",
    "print(f\"Longest text: {max(all_text_lengths)} characters\")\n",
    "\n",
    "print_section(\"DATA QUALITY\")\n",
    "demo_missing = df_demo.isnull().sum().sum()\n",
    "identity_missing = df_identity.isnull().sum().sum()\n",
    "print(f\"Missing values in demographic dataset: {demo_missing}\")\n",
    "print(f\"Missing values in identity dataset: {identity_missing}\")\n",
    "print(f\"Total missing values: {demo_missing + identity_missing}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"                    PROFILING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
