facebook:
  testing:
    template: |
      Describe briefly this image from the perspective of the following persona: [PERSONA].
  v7:
    template: |
      Please analyze this meme and provide exactly three lines in this format:
      1. Harm classification (exactly one word): 'hateful' or 'not_hateful'
      2. Protected categories targeted (comma-separated list from): race, disability, religion, nationality, sex, pc_empty
      3. Attack types used (comma-separated list from): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence, attack_empty

      Respond taking on the perspective of the following persona: [PERSONA].
    placeholders:
      - PERSONA
  v2:
    template: |
      Your task is to analyze the provided meme.
      In doing so, you are taking on the perspective of the following persona: [PERSONA].

      Given this context, please analyze this meme and provide exactly three lines in this format:
      1. Harm classification (exactly one word): 'hateful' or 'not_hateful'
      2. Protected categories targeted (comma-separated list from): race, disability, religion, nationality, sex, pc_empty
      3. Attack types used (comma-separated list from): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence, attack_empty
    placeholders:
      - PERSONA
  v3:
    template: |
      Your task is to analyze the provided meme.
      In doing so, you are taking on the perspective of the following persona: [PERSONA].

      Based on your analysis of the meme, output a single comma-separated line with at least one protected category (race, disability, religion, nationality, sex, pc_none) and at least one attack type (contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence, attack_none)
    placeholders:
      - PERSONA
  v4:
    template: |
      Your task is to analyze the provided meme.
      In doing so, you are taking on the perspective of the following persona: [PERSONA].

      Based on your analysis of the meme, output a single comma-separated line with either "pc_empty" or at least one protected category ("disability", "race", religion", nationality", "sex") paired with at least one corresponding attack type ("contempt", "mocking", "inferiority", "slurs", "exclusion", "dehumanizing", "inciting_violence", "attack_empty")
    placeholders:
      - PERSONA
  v5b:
    template: |
      Taking on the perspective of [PERSONA], analyze this meme in two steps:

      1. First, classify if this meme is harmful or not:
         Answer with only: HARMFUL or NOT_HARMFUL

      2. If you answered HARMFUL:
         Targeted groups affected (TG:) (minimum 1): disability, race, religion, nationality, sex
         Attack methods used (AM:) (minimum 1): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence
    placeholders:
      - PERSONA
  v6:
    template: |
      Taking on the perspective of [PERSONA], analyze this meme in two steps:

      1. First, classify if this meme is harmful or not:
         Answer with only: HARMFUL or NOT_HARMFUL

      2. If you answered HARMFUL:
         Targeted groups affected (TG:) (choose 1): disability, race, religion, nationality, sex
         Attack methods used (AM:) (choose 1): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence

      When executing this task, think carefully about how a person with the described perspective would interpret the meme.
    placeholders:
      - PERSONA
  v1:
    template: |
      You are an expert content analyst specializing in identifying hate. Your task is to analyze the provided meme strictly from the perspective of [PERSONA].

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Analysis Logic:**
      1.  **Perspective:** Embody the given persona. Your entire analysis must reflect their likely values, biases, worldview and sensitivity level to hateful content
      2.  **Classification:** First, determine if the persona would consider the meme to be hate speech. This determines the value of `is_hate_speech`.
      3.  **Conditional Analysis:**
          - **If `is_hate_speech` is "true"**: You MUST identify the single, *most prominent* `target_group` and the single, *primary* `attack_method` used.
          - **If `is_hate_speech` is "false"**: The `target_group` and `attack_method` fields MUST be `null`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].
      - `target_group` (string or null): If hateful, must be one of ["disability", "race", "religion", "nationality", "sex"]. Otherwise, it must be `null`.
      - `attack_method` (string or null): If hateful, must be one of ["contempt", "mocking", "inferiority", "slurs", "exclusion", "dehumanizing", "inciting_violence"]. Otherwise, it must be `null`.

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true",
        "target_group": "race",
        "attack_method": "mocking"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false",
        "target_group": null,
        "attack_method": null
      }

      Persona: [PERSONA]

      Now, analyze the image from the persona's perspective and generate the JSON object.
    placeholders:
      - PERSONA

hatexplain:
  v1:
    template: |
      Taking on the perspective of [PERSONA], analyze this text in two steps:

      1. First, determine if this text is hate speech, offensive, or normal:
         Answer with only: HATE_SPEECH, OFFENSIVE, or NORMAL

      2. If you answered HATE_SPEECH or OFFENSIVE:
         Targeted groups affected (TG:) (choose 1): race, religion, gender, sexuality, disability, minority, other, economic, refugee, none
         Attack methods used (AM:) (choose 1): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence

      When executing this task, think carefully about how a person with the described perspective would interpret the meme.
    placeholders:
      - PERSONA

mmhs150k:
  v0:
    template: |
      The tweet provided is composed of the given image and the following text:
      [TWEET_TEXT]
      Taking on the perspective of [PERSONA], analyze this tweet in two steps:
      1. First, classify if this tweet is harmful or not:
       Answer with only: HARMFUL or NOT_HARMFUL
      2. If classification is HARMFUL, provide:
       TG (choose 1): Racist, Sexist, Homophobe, Religion, OtherHate.
    placeholders:
      - TWEET_TEXT
      - PERSONA
  v1:
    template: |
      The tweet provided is composed of the given image and the following text:
      [TWEET_TEXT]
      Taking on the perspective of [PERSONA], analyze this tweet in two steps:
      1. First, classify if this meme is harmful or not:
       Answer with only: HARMFUL or NOT_HARMFUL
      2. If classification is HARMFUL, provide:
       TG (choose 1): Racist, Sexist, Homophobe, Religion, OtherHate.
      When executing this task, think carefully about how a person with the described perspective would interpret the tweet.
    placeholders:
      - TWEET_TEXT
      - PERSONA
  v2:
    template: |
      The tweet provided is composed of the given image and the following text:
      [TWEET_TEXT]
      Taking on the perspective of [PERSONA], analyze this tweet in two steps:
      1. First, classify if this meme is harmful or not:
       Output format: CLASSIFICATION: [HARMFUL or NOT_HARMFUL]
      2. If classification is HARMFUL, you must also specify the target group:
       Output format: TARGET_GROUP: [Racist|Sexist|Homophobe|Religion|OtherHate]
      Your output must follow these formats exactly. If classification is HARMFUL, both lines are required.
      When executing this task, think carefully about how a person with the described perspective would interpret the tweet.
    placeholders:
      - TWEET_TEXT
      - PERSONA

facebook_harmful_memes_left_right:
  leftmost:
    template: |
      Analyze this meme taking on the perspective of the following politically left leaning persona:
      [PERSONA].
      Analyze this meme in two steps:
      1. First, classify if this meme is harmful or not:
       Answer with only: HARMFUL or NOT_HARMFUL
      2. If you answered HARMFUL:
       Targeted groups affected (TG:) (choose 1): disability, race, religion, nationality, sex
       Attack methods used (AM:) (choose 1): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence
      When executing this task, think carefully about how a person with the described left leaning perspective would interpret the meme.
    placeholders:
      - PERSONA
    political_orientation: left
  rightmost:
    template: |
      Analyze this meme taking on the perspective of the following politically right leaning persona:
      [PERSONA].
      Analyze this meme in two steps:
      1. First, classify if this meme is harmful or not:
       Answer with only: HARMFUL or NOT_HARMFUL
      2. If you answered HARMFUL:
       Targeted groups affected (TG:) (choose 1): disability, race, religion, nationality, sex
       Attack methods used (AM:) (choose 1): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence
      When executing this task, think carefully about how a person with the described right leaning perspective would interpret the meme.
    placeholders:
      - PERSONA
    political_orientation: right

mmhs150k_left_right:
  leftmost:
    template: |
      The tweet provided is composed of the given image and the following text:
      [TWEET_TEXT]
      Taking on the perspective of a left leaning [PERSONA], analyze this tweet in two steps:
      1. First, classify if this meme is harmful or not:
       Output format: CLASSIFICATION: [HARMFUL or NOT_HARMFUL]
      2. If classification is HARMFUL, you must also specify the target group:
       Output format: TARGET_GROUP: [Racist|Sexist|Homophobe|Religion|OtherHate]
      Your output must follow these formats exactly. If classification is HARMFUL, both lines are required.
      When executing this task, think carefully about how a person with the described left leaning perspective would interpret the tweet.
    placeholders:
      - TWEET_TEXT
      - PERSONA
    political_orientation: left
  rightmost:
    template: |
      The tweet provided is composed of the given image and the following text:
      [TWEET_TEXT]
      Taking on the perspective of a right leaning [PERSONA], analyze this tweet in two steps:
      1. First, classify if this meme is harmful or not:
       Output format: CLASSIFICATION: [HARMFUL or NOT_HARMFUL]
      2. If classification is HARMFUL, you must also specify the target group:
       Output format: TARGET_GROUP: [Racist|Sexist|Homophobe|Religion|OtherHate]
      Your output must follow these formats exactly. If classification is HARMFUL, both lines are required.
      When executing this task, think carefully about how a person with the described right leaning perspective would interpret the tweet.
    placeholders:
      - TWEET_TEXT
      - PERSONA
    political_orientation: right

hate_speech_text:
  v1:
    template: |
      Here is a text that may contain hate speech: [TEXT].
      Taking on the perspective of the following persona: [PERSONA], generate a JSON output to classify this text saying if it is_hate_speech, the target_category affected and the specific_target being attacked.
    placeholders:
      - TEXT
      - PERSONA

yoder:
  v1:
    template: |
      You are an expert content analyst specializing in identifying hate speech according to specific guidelines. Your task is to analyze the provided text strictly from the perspective of [PERSONA].

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Analysis Logic:**
      1.  **Perspective:** Embody the given persona. Your entire analysis must reflect their likely values, biases, worldview and sensitivity level to hateful content.
      2.  **Classification:** First, determine if the persona would consider the text to be hate speech. This determines the value of `is_hate_speech`.
      3.  **Conditional Analysis:**
          - **If `is_hate_speech` is "true"**: You MUST identify the single, primary `target_category` being attacked.
          - **If `is_hate_speech` is "false"**: The `target_category` field MUST be `null`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].
      - `target_group` (string or null): If hateful, must be one of ["women", "black", "lgbtq+", "muslims/arabic", "asian", "latino/hispanic", "jews", "white", "men", "christians"]. If not hateful, it must be `null`.

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true",
        "target_category": "latino/hispanic"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false",
        "target_category": "none"
      }

      Persona: [PERSONA]
      Text: [TEXT]
      
      Now, analyze the text from the persona's perspective and generate the JSON object.
    placeholders:
      - TEXT
      - PERSONA
  v2:
    template: |
      Here is a text that may contain hate speech: [TEXT].
      Taking on the perspective of the following persona: [PERSONA], generate a JSON output to classify this text saying if it is_hate_speech and if it is, indicate also the effected target_category being attacked.
    placeholders:
        - TEXT
        - PERSONA
subdata:
  v1:
    template: |
      You are an expert content analyst specializing in identifying hate speech according to specific guidelines. Your task is to analyze the provided text strictly from the perspective of [PERSONA].

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Analysis Logic:**
      1.  **Perspective:** Embody the given persona. Your entire analysis must reflect their likely values, biases, worldview and sensitivity level to hateful content.
      2.  **Classification:** Determine if the persona would consider the text to be hate speech. This determines the value of `is_hate_speech`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false"
      }

      Persona: [PERSONA]
      Text: [TEXT]
      
      Now, analyze the text from the persona's perspective and generate the JSON object.
multioff:
  v1:
    template: |
      You are an expert content analyst specializing in identifying hate speech according to specific guidelines. Your task is to analyze the provided meme strictly from the perspective of [PERSONA].

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Analysis Logic:**
      1.  **Perspective:** Embody the given persona. Your entire analysis must reflect their likely values, biases, worldview and sensitivity level to hateful content.
      2.  **Classification:** Determine if the persona would consider the meme to be hate speech. This determines the value of `is_hate_speech`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false"
      }

      Persona: [PERSONA]
      
      Now, analyze the meme from the persona's perspective and generate the JSON object.