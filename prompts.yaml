facebook:
  testing:
    template: |
      Describe briefly this image from the perspective of the following persona: [PERSONA].
  v7:
    template: |
      Please analyze this meme and provide exactly three lines in this format:
      1. Harm classification (exactly one word): 'hateful' or 'not_hateful'
      2. Protected categories targeted (comma-separated list from): race, disability, religion, nationality, sex, pc_empty
      3. Attack types used (comma-separated list from): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence, attack_empty

      Respond taking on the perspective of the following persona: [PERSONA].
    placeholders:
      - PERSONA
  v2:
    template: |
      Your task is to analyze the provided meme.
      In doing so, you are taking on the perspective of the following persona: [PERSONA].

      Given this context, please analyze this meme and provide exactly three lines in this format:
      1. Harm classification (exactly one word): 'hateful' or 'not_hateful'
      2. Protected categories targeted (comma-separated list from): race, disability, religion, nationality, sex, pc_empty
      3. Attack types used (comma-separated list from): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence, attack_empty
    placeholders:
      - PERSONA
  v3:
    template: |
      Your task is to analyze the provided meme.
      In doing so, you are taking on the perspective of the following persona: [PERSONA].

      Based on your analysis of the meme, output a single comma-separated line with at least one protected category (race, disability, religion, nationality, sex, pc_none) and at least one attack type (contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence, attack_none)
    placeholders:
      - PERSONA
  v4:
    template: |
      Your task is to analyze the provided meme.
      In doing so, you are taking on the perspective of the following persona: [PERSONA].

      Based on your analysis of the meme, output a single comma-separated line with either "pc_empty" or at least one protected category ("disability", "race", religion", nationality", "sex") paired with at least one corresponding attack type ("contempt", "mocking", "inferiority", "slurs", "exclusion", "dehumanizing", "inciting_violence", "attack_empty")
    placeholders:
      - PERSONA
  v5b:
    template: |
      Taking on the perspective of [PERSONA], analyze this meme in two steps:

      1. First, classify if this meme is harmful or not:
         Answer with only: HARMFUL or NOT_HARMFUL

      2. If you answered HARMFUL:
         Targeted groups affected (TG:) (minimum 1): disability, race, religion, nationality, sex
         Attack methods used (AM:) (minimum 1): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence
    placeholders:
      - PERSONA
  v6:
    template: |
      Taking on the perspective of [PERSONA], analyze this meme in two steps:

      1. First, classify if this meme is harmful or not:
         Answer with only: HARMFUL or NOT_HARMFUL

      2. If you answered HARMFUL:
         Targeted groups affected (TG:) (choose 1): disability, race, religion, nationality, sex
         Attack methods used (AM:) (choose 1): contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting_violence

      When executing this task, think carefully about how a person with the described perspective would interpret the meme.
    placeholders:
      - PERSONA
  v1:
    template: |
      You are an expert content analyst specializing in identifying hate. Your task is to analyze the provided meme strictly from the perspective of [PERSONA].

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Analysis Logic:**
      1.  **Perspective:** Embody the given persona. Your entire analysis must reflect their likely values, biases, worldview and sensitivity level to hateful content
      2.  **Classification:** First, determine if the persona would consider the meme to be hate speech. This determines the value of `is_hate_speech`.
      3.  **Conditional Analysis:**
          - **If `is_hate_speech` is "true"**: You MUST identify the single, *most prominent* `target_group` and the single, *primary* `attack_method` used.
          - **If `is_hate_speech` is "false"**: The `target_group` and `attack_method` fields MUST be `null`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].
      - `target_group` (string or null): If hateful, must be one of ["disability", "race", "religion", "nationality", "sex"]. Otherwise, it must be `null`.
      - `attack_method` (string or null): If hateful, must be one of ["contempt", "mocking", "inferiority", "slurs", "exclusion", "dehumanizing", "inciting_violence"]. Otherwise, it must be `null`.

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true",
        "target_group": "race",
        "attack_method": "mocking"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false",
        "target_group": null,
        "attack_method": null
      }

      Persona: [PERSONA]

      Now, analyze the image from the persona's perspective and generate the JSON object.
    placeholders:
      - PERSONA
  nopersona:
    template: |
      You are an expert content analyst specializing in identifying hate. Your task is to analyze the provided meme.

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Analysis Logic:**
      1.  **Classification:** First, determine if the meme is hate speech. This determines the value of `is_hate_speech`.
      2.  **Conditional Analysis:**
          - **If `is_hate_speech` is "true"**: You MUST identify the single, *most prominent* `target_group` and the single, *primary* `attack_method` used.
          - **If `is_hate_speech` is "false"**: The `target_group` and `attack_method` fields MUST be `null`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].
      - `target_group` (string or null): If hateful, must be one of ["disability", "race", "religion", "nationality", "sex"]. Otherwise, it must be `null`.
      - `attack_method` (string or null): If hateful, must be one of ["contempt", "mocking", "inferiority", "slurs", "exclusion", "dehumanizing", "inciting_violence"]. Otherwise, it must be `null`.

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true",
        "target_group": "race",
        "attack_method": "mocking"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false",
        "target_group": null,
        "attack_method": null
      }

      Now, analyze the image and generate the JSON object.


# hate_speech_text:
#   v1:
#     template: |
#       Here is a text that may contain hate speech: [TEXT].
#       Taking on the perspective of the following persona: [PERSONA], generate a JSON output to classify this text saying if it is_hate_speech, the target_category affected and the specific_target being attacked.
#     placeholders:
#       - TEXT
#       - PERSONA

yoder:
  v1:
    template: |
      You are an expert content analyst specializing in identifying hate speech according to specific guidelines. Your task is to analyze the provided text strictly from the perspective of [PERSONA].

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Analysis Logic:**
      1.  **Perspective:** Embody the given persona. Your entire analysis must reflect their likely values, biases, worldview and sensitivity level to hateful content.
      2.  **Classification:** First, determine if the persona would consider the text to be hate speech. This determines the value of `is_hate_speech`.
      3.  **Conditional Analysis:**
          - **If `is_hate_speech` is "true"**: You MUST identify the single, primary `target_category` being attacked.
          - **If `is_hate_speech` is "false"**: The `target_category` field MUST be `null`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].
      - `target_group` (string or null): If hateful, must be one of ["women", "black", "lgbtq+", "muslims/arabic", "asian", "latino/hispanic", "jews", "white", "men", "christians"]. If not hateful, it must be `null`.

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true",
        "target_category": "latino/hispanic"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false",
        "target_category": "none"
      }

      Persona: [PERSONA]
      Text: [TEXT]
      
      Now, analyze the text from the persona's perspective and generate the JSON object.
    placeholders:
      - TEXT
      - PERSONA
  nopersona:
    template: |
      You are an expert content analyst specializing in identifying hate speech according to specific guidelines. Your task is to analyze the provided text.

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Analysis Logic:**
      1.  **Classification:** First, determine if the text is hate speech. This determines the value of `is_hate_speech`.
      2.  **Conditional Analysis:**
          - **If `is_hate_speech` is "true"**: You MUST identify the single, primary `target_category` being attacked.
          - **If `is_hate_speech` is "false"**: The `target_category` field MUST be `null`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].
      - `target_group` (string or null): If hateful, must be one of ["women", "black", "lgbtq+", "muslims/arabic", "asian", "latino/hispanic", "jews", "white", "men", "christians"]. If not hateful, it must be `null`.

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true",
        "target_category": "latino/hispanic"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false",
        "target_category": "none"
      }

      Text: [TEXT]
      
      Now, analyze the text and generate the JSON object.
    placeholders:
      - TEXT
  v2:
    template: |
      Here is a text that may contain hate speech: [TEXT].
      Taking on the perspective of the following persona: [PERSONA], generate a JSON output to classify this text saying if it is_hate_speech and if it is, indicate also the effected target_category being attacked.
    placeholders:
        - TEXT
        - PERSONA
subdata:
  v1:
    template: |
      You are an expert content analyst specializing in identifying hate speech according to specific guidelines. Your task is to analyze the provided text strictly from the perspective of [PERSONA].

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Analysis Logic:**
      1.  **Perspective:** Embody the given persona. Your entire analysis must reflect their likely values, biases, worldview and sensitivity level to hateful content.
      2.  **Classification:** Determine if the persona would consider the text to be hate speech. This determines the value of `is_hate_speech`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false"
      }

      Persona: [PERSONA]
      Text: [TEXT]
      
      Now, analyze the text from the persona's perspective and generate the JSON object.
  nopersona:
    template: |
      You are an expert content analyst specializing in identifying hate speech according to specific guidelines. Your task is to analyze the provided text.

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Classification Logic:** Determine if the text is hate speech. This determines the value of `is_hate_speech`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false"
      }

      Text: [TEXT]
      
      Now, analyze the text and generate the JSON object.
multioff:
  v1:
    template: |
      You are an expert content analyst specializing in identifying hate speech according to specific guidelines. Your task is to analyze the provided meme strictly from the perspective of [PERSONA].

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Analysis Logic:**
      1.  **Perspective:** Embody the given persona. Your entire analysis must reflect their likely values, biases, worldview and sensitivity level to hateful content.
      2.  **Classification:** Determine if the persona would consider the meme to be hate speech. This determines the value of `is_hate_speech`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false"
      }

      Persona: [PERSONA]
      
      Now, analyze the meme from the persona's perspective and generate the JSON object.
  nopersona:
    template: |
      You are an expert content analyst specializing in identifying hate speech according to specific guidelines. Your task is to analyze the provided meme.

      Based on your analysis, you must generate a single, valid JSON object that adheres to the following schema and logic. Do not output any text other than the JSON object.

      **Classification Logic: ** Determine if the meme is hate speech. This determines the value of `is_hate_speech`.

      **JSON Schema:**
      - `is_hate_speech` (string): Must be one of ["true", "false"].

      **Example 1 (Hateful):**
      {
        "is_hate_speech": "true"
      }

      **Example 2 (Not Hateful):**
      {
        "is_hate_speech": "false"
      }

      Now, analyze the meme and generate the JSON object.